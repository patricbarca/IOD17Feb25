{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XebDJ3UnS3n3"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_-HjrL6S3n5"
   },
   "source": [
    "# Lab 6.4\n",
    "# *PCA Lab*\n",
    "\n",
    "**In this lab, we will:**\n",
    "- Explore how PCA is related to correlation.\n",
    "- Use PCA to perform dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSj3bDwfZKjj"
   },
   "source": [
    "### 1. Load Data\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "\n",
    "Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "All feature values are recoded with four significant digits.\n",
    "\n",
    "Missing attribute values: none\n",
    "\n",
    "Class distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T05:10:33.642287Z",
     "start_time": "2019-05-16T05:10:33.429626Z"
    },
    "id": "YC6yX8_lZKjl"
   },
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T05:10:35.890574Z",
     "start_time": "2019-05-16T05:10:35.812753Z"
    },
    "id": "-ifYPbk3ZKjr"
   },
   "outputs": [],
   "source": [
    "breast_cancer_csv = r'C:\\Users\\pabarca\\OneDrive - GRUPO GRANSOLAR\\Desktop\\IOD - Python\\DATA\\breast-cancer-wisconsin-data.csv'\n",
    "breast_cancer = pd.read_csv(breast_cancer_csv, index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBV7wJrhZKjt"
   },
   "source": [
    "### 2. EDA\n",
    "\n",
    "Explore dataset. Clean data. Find correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer.drop(labels='Unnamed: 32', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer['diagnosis'].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied code from seaborn examples\n",
    "# https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(breast_cancer.corr(numeric_only=True), dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(18, 18))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(breast_cancer.corr(numeric_only=True), mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:10:16.792504Z",
     "start_time": "2019-05-16T01:10:16.786523Z"
    },
    "id": "Chhz61HiZKju"
   },
   "source": [
    "### 3. Subset & Normalise\n",
    "\n",
    "Subset the data to only include all columns except diagnosis, then apply StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffe2XkVjZKjv"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target column name\n",
    "target_column = 'diagnosis'\n",
    "\n",
    "# Save feature column names as a list\n",
    "feature_columns = breast_cancer.columns.drop('diagnosis')\n",
    "#feature_columns = [c for c in breast_cancer.columns if c != 'diagnosis']\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = breast_cancer[feature_columns]\n",
    "y = breast_cancer['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to fit and transform X to be standardised\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy5nd94nZKjy"
   },
   "source": [
    "### Calculate correlation matrix\n",
    "\n",
    "We will be using the correlation matrix to calculate the eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bemSCz5HZKjz"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "pd.DataFrame(Xs, columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe from Xs and calculate correlation matrix with .corr() method\n",
    "\n",
    "Xcorr = pd.DataFrame(Xs, columns=feature_columns).corr()\n",
    "Xcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Xcorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJtgs5K7ZKj2"
   },
   "outputs": [],
   "source": [
    "# Calculate eigenvalues and eigenvectors of correlation matrix\n",
    "eig_vals, eig_vecs = np.linalg.eig(Xcorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first eigenvalue\n",
    "eig_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the corresponding eigenvector\n",
    "eig_vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7kSQdCgZKj5"
   },
   "source": [
    "### 5. Calculate and plot the explained variance\n",
    "\n",
    "A useful measure is the **explained variance**, which is calculated from the eigenvalues.\n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component.\n",
    "\n",
    "### $$ ExpVar_i = \\bigg(\\frac{eigenvalue_i}{\\sum_j^n{eigenvalue_j}}\\bigg) * 100$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFcv-H6XZKj5"
   },
   "outputs": [],
   "source": [
    "def calculate_cum_var_exp(eig_vals):\n",
    "    tot = sum(eig_vals)\n",
    "    #var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "    #var_exp = [(i / tot)*100 for i in eig_vals]\n",
    "    var_exp = []\n",
    "    for i in eig_vals:\n",
    "        var_i = (i / tot)*100\n",
    "        var_exp.append(var_i)\n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "    return cum_var_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BB22fsA_ZKj8"
   },
   "outputs": [],
   "source": [
    "def plot_var_exp(eig_vals):\n",
    "\n",
    "    cum_var_exp = calculate_cum_var_exp(eig_vals)\n",
    "\n",
    "    plt.figure(figsize=(9,7))\n",
    "\n",
    "    component_number = [i+1 for i in range(len(cum_var_exp))]\n",
    "\n",
    "    plt.plot(component_number, cum_var_exp, lw=7)\n",
    "\n",
    "    plt.axhline(y=0, linewidth=5, color='grey', ls='dashed')\n",
    "    plt.axhline(y=100, linewidth=3, color='grey', ls='dashed')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([1,30])\n",
    "    ax.set_ylim([-5,105])\n",
    "\n",
    "    ax.set_ylabel('cumulative variance explained', fontsize=16)\n",
    "    ax.set_xlabel('component', fontsize=16)\n",
    "\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(12)\n",
    "\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(12)\n",
    "\n",
    "    ax.set_title('component vs cumulative variance explained\\n', fontsize=20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var_exp(eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDUAFYjTZKj-"
   },
   "source": [
    "### 6. Using sklearn For PCA\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "- Create an instance of PCA\n",
    "    - Fit X\n",
    "- Plot the cumulative explained variance\n",
    "- Apply dimensionality reduction to X with n_components=16\n",
    "    - Fit and transform X\n",
    "- Create a pairplot of PCA-transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmM5y7wyZKj_"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Create an instance of PCA (do not set n_components)\n",
    "\n",
    "# Fit Xs (breast cancer dataset having standardised features)\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PCA class\n",
    "breast_pca = PCA()\n",
    "\n",
    "# Fit PCA with standardised features\n",
    "breast_pca.fit(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsXV-pGrZKkB"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Plot cumulative variance explained vs number of components using the plot_var_exp function from above\n",
    "plt.plot(range(1, 31), 100 - (100*breast_pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained cumulative variance %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative variance explained vs number of components with custom plot_var_exp function\n",
    "plot_var_exp(breast_pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-PBgu7WZKkF"
   },
   "outputs": [],
   "source": [
    "# Create another instance of PCA (this time with n_components = 16)\n",
    "breast_pca = PCA(n_components=16)\n",
    "\n",
    "# Fit PCA with standardised features\n",
    "breast_pca.fit(Xs)\n",
    "std_x_pca = breast_pca.transform(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLq_Z7CeZKkJ"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Create a pairplot of PCA-transformed data\n",
    "# Show principal components as a dataframe\n",
    "\n",
    "pd.DataFrame(std_x_pca).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Xs, columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(std_x_pca), kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PC1 vs PC2\n",
    "plt.scatter(std_x_pca[:, 0], std_x_pca[:, 1])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46-UYVBqencm"
   },
   "source": [
    "You should notice that the transformed features have been decorrelated (neither increasing nor decreasing trends in pairs of variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Mt2CQyZKkM"
   },
   "source": [
    "### 7. Split Data to 80/20 and use PCA prior to a supervised learning task\n",
    "\n",
    "In this section we use PCA as a preprocessing step to a supervised learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2cdhvz_Japz"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktkhaOFuJvfU"
   },
   "source": [
    "Split the original dataset 80/20. Then apply standard scaler followed by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFUL4zEPZKkN"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply standard scaler to X_train and X_test (fit_transform on X_train, transform on X_test):\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate the PCA class and set at 16 components\n",
    "breast_pca = PCA(n_components=16)\n",
    "\n",
    "# Apply PCA to the standardised features\n",
    "X_train_scaled_pca = breast_pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled_pca = breast_pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7SjCEMaKYVf"
   },
   "source": [
    "Apply a KNN algorithm on `X_train_scaled` and `X_train_scaled_pca` with 5 neighbours, then evaluate using `X_test_scaled` and `X_test_scaled_pca`. Has performance been impacted as a result of dimension reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPHhyfeDZKkP"
   },
   "outputs": [],
   "source": [
    "# Set KNN classifier to use 5 neighbors and fit to X_train_scaled\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test accuracy of KNN using standardised data\n",
    "print(\"Number of features in standardised data:       \", X_test_scaled.shape[1])\n",
    "print(\"Test accuracy using standardised data:    \", knn5.score(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW3q2bErKidQ"
   },
   "outputs": [],
   "source": [
    "# Set KNN classifier to use 5 neighbors and fit to X_train_scaled_pca\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_train_scaled_pca, y_train)\n",
    "\n",
    "# Test accuracy of KNN using standardised PCA-transformed data\n",
    "print(\"Number of features in standardised PCA-transformed data:       \", X_test_scaled_pca.shape[1])\n",
    "print(\"Test accuracy using standardised PCA-transformed data:    \", knn5.score(X_test_scaled_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5ThN4pKZKkR"
   },
   "source": [
    "**References**\n",
    "\n",
    "[Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/downloads/breast-cancer-wisconsin-data.zip/2)\n",
    "\n",
    "[Breast Cancer Machine Learning Prediction](https://gtraskas.github.io/post/breast_cancer/)\n",
    "\n",
    "[Understanding PCA (Principal Component Analysis) with Python](https://towardsdatascience.com/dive-into-pca-principal-component-analysis-with-python-43ded13ead21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
