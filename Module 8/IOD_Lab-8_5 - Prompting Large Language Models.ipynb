{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {
    "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {
    "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612"
   },
   "source": [
    "# Lab 8.5 - Prompting Large Language Models\n",
    "\n",
    "In this lab we will practise prompting with a few Large Language Models (LLMs) using Groq (not to be confused with Grok). Groq is a platform that provides access to their custom-built AI hardware via APIs, allowing users to run open-source models such as Llama.\n",
    "\n",
    "We shall see that while LLMs are powerful tools, how you ask a question or frame a task can dramatically influence the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {
    "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {
    "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719"
   },
   "source": [
    "Step 1: Sign up for a free Groq account at https://console.groq.com/home .\n",
    "\n",
    "Step 2: Create a new API key at https://console.groq.com/keys. Copy-paste it into an empty text file called 'groq_key.txt'.\n",
    "\n",
    "Running the next cell will then read in this key and assign it to the variable `groq_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {
    "id": "56441b82-ddc5-46e9-9583-878e8a807b7f"
   },
   "outputs": [],
   "source": [
    "groqfilename = r'C:\\Users\\pabarca\\OneDrive - GRUPO GRANSOLAR\\Desktop\\IOD - Python\\DATA\\groq.txt' # this file contains a single line containing your Groq API key only\n",
    "try:\n",
    "    with open(groqfilename, 'r') as f:\n",
    "        groq_key = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88",
   "metadata": {
    "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\pabarca\\appdata\\local\\anaconda1\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c",
   "metadata": {
    "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227",
   "metadata": {
    "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227"
   },
   "source": [
    "First create an instance of the Groq client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b",
   "metadata": {
    "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b"
   },
   "outputs": [],
   "source": [
    "client = Groq(api_key=groq_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097cd77-7a8d-4502-ae6e-607782706b26",
   "metadata": {
    "id": "a097cd77-7a8d-4502-ae6e-607782706b26"
   },
   "source": [
    "The following code shows what models are currently accessible through Groq. `context_window` refers to the size of memory (in tokens) during a session and `max_completion_tokens` is the maximum number of tokens that are generated in an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c292b0f-caab-4166-881e-f91a94a959f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'list', 'data': [{'id': 'compound-beta', 'object': 'model', 'created': 1740880017, 'owned_by': 'Groq', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'meta-llama/llama-guard-4-12b', 'object': 'model', 'created': 1746743847, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 1024}, {'id': 'playai-tts', 'object': 'model', 'created': 1740682771, 'owned_by': 'PlayAI', 'active': True, 'context_window': 8192, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'mistral-saba-24b', 'object': 'model', 'created': 1739996492, 'owned_by': 'Mistral AI', 'active': True, 'context_window': 32768, 'public_apps': None, 'max_completion_tokens': 32768}, {'id': 'whisper-large-v3-turbo', 'object': 'model', 'created': 1728413088, 'owned_by': 'OpenAI', 'active': True, 'context_window': 448, 'public_apps': None, 'max_completion_tokens': 448}, {'id': 'compound-beta-mini', 'object': 'model', 'created': 1742953279, 'owned_by': 'Groq', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'object': 'model', 'created': 1743877158, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'meta-llama/llama-prompt-guard-2-22m', 'object': 'model', 'created': 1748632101, 'owned_by': 'Meta', 'active': True, 'context_window': 512, 'public_apps': None, 'max_completion_tokens': 512}, {'id': 'llama3-8b-8192', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'whisper-large-v3', 'object': 'model', 'created': 1693721698, 'owned_by': 'OpenAI', 'active': True, 'context_window': 448, 'public_apps': None, 'max_completion_tokens': 448}, {'id': 'playai-tts-arabic', 'object': 'model', 'created': 1740682783, 'owned_by': 'PlayAI', 'active': True, 'context_window': 8192, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'meta-llama/llama-4-scout-17b-16e-instruct', 'object': 'model', 'created': 1743874824, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'allam-2-7b', 'object': 'model', 'created': 1737672203, 'owned_by': 'SDAIA', 'active': True, 'context_window': 4096, 'public_apps': None, 'max_completion_tokens': 4096}, {'id': 'meta-llama/llama-prompt-guard-2-86m', 'object': 'model', 'created': 1748632165, 'owned_by': 'Meta', 'active': True, 'context_window': 512, 'public_apps': None, 'max_completion_tokens': 512}, {'id': 'deepseek-r1-distill-llama-70b', 'object': 'model', 'created': 1737924940, 'owned_by': 'DeepSeek / Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 131072}, {'id': 'llama3-70b-8192', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'llama-3.1-8b-instant', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 131072}, {'id': 'distil-whisper-large-v3-en', 'object': 'model', 'created': 1693721698, 'owned_by': 'Hugging Face', 'active': True, 'context_window': 448, 'public_apps': None, 'max_completion_tokens': 448}, {'id': 'gemma2-9b-it', 'object': 'model', 'created': 1693721698, 'owned_by': 'Google', 'active': True, 'context_window': 8192, 'public_apps': None, 'max_completion_tokens': 8192}, {'id': 'qwen/qwen3-32b', 'object': 'model', 'created': 1748396646, 'owned_by': 'Alibaba Cloud', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 40960}, {'id': 'llama-3.3-70b-versatile', 'object': 'model', 'created': 1733447754, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 32768}, {'id': 'qwen-qwq-32b', 'object': 'model', 'created': 1741214760, 'owned_by': 'Alibaba Cloud', 'active': True, 'context_window': 131072, 'public_apps': None, 'max_completion_tokens': 131072}]}\n"
     ]
    }
   ],
   "source": [
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58035c8b-44ee-4611-bc10-520587cbf300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               id object     created  \\\n",
      "13            meta-llama/llama-prompt-guard-2-86m  model  1748632165   \n",
      "7             meta-llama/llama-prompt-guard-2-22m  model  1748632101   \n",
      "19                                 qwen/qwen3-32b  model  1748396646   \n",
      "1                    meta-llama/llama-guard-4-12b  model  1746743847   \n",
      "6   meta-llama/llama-4-maverick-17b-128e-instruct  model  1743877158   \n",
      "11      meta-llama/llama-4-scout-17b-16e-instruct  model  1743874824   \n",
      "5                              compound-beta-mini  model  1742953279   \n",
      "21                                   qwen-qwq-32b  model  1741214760   \n",
      "0                                   compound-beta  model  1740880017   \n",
      "10                              playai-tts-arabic  model  1740682783   \n",
      "2                                      playai-tts  model  1740682771   \n",
      "3                                mistral-saba-24b  model  1739996492   \n",
      "14                  deepseek-r1-distill-llama-70b  model  1737924940   \n",
      "12                                     allam-2-7b  model  1737672203   \n",
      "20                        llama-3.3-70b-versatile  model  1733447754   \n",
      "4                          whisper-large-v3-turbo  model  1728413088   \n",
      "8                                  llama3-8b-8192  model  1693721698   \n",
      "9                                whisper-large-v3  model  1693721698   \n",
      "17                     distil-whisper-large-v3-en  model  1693721698   \n",
      "16                           llama-3.1-8b-instant  model  1693721698   \n",
      "15                                llama3-70b-8192  model  1693721698   \n",
      "18                                   gemma2-9b-it  model  1693721698   \n",
      "\n",
      "           owned_by  active  context_window public_apps  max_completion_tokens  \n",
      "13             Meta    True             512        None                    512  \n",
      "7              Meta    True             512        None                    512  \n",
      "19    Alibaba Cloud    True          131072        None                  40960  \n",
      "1              Meta    True          131072        None                   1024  \n",
      "6              Meta    True          131072        None                   8192  \n",
      "11             Meta    True          131072        None                   8192  \n",
      "5              Groq    True          131072        None                   8192  \n",
      "21    Alibaba Cloud    True          131072        None                 131072  \n",
      "0              Groq    True          131072        None                   8192  \n",
      "10           PlayAI    True            8192        None                   8192  \n",
      "2            PlayAI    True            8192        None                   8192  \n",
      "3        Mistral AI    True           32768        None                  32768  \n",
      "14  DeepSeek / Meta    True          131072        None                 131072  \n",
      "12            SDAIA    True            4096        None                   4096  \n",
      "20             Meta    True          131072        None                  32768  \n",
      "4            OpenAI    True             448        None                    448  \n",
      "8              Meta    True            8192        None                   8192  \n",
      "9            OpenAI    True             448        None                    448  \n",
      "17     Hugging Face    True             448        None                    448  \n",
      "16             Meta    True          131072        None                 131072  \n",
      "15             Meta    True            8192        None                   8192  \n",
      "18           Google    True            8192        None                   8192  \n"
     ]
    }
   ],
   "source": [
    "response_json = response.json()\n",
    "if 'data' in response_json:\n",
    "    df = pd.DataFrame(response_json['data']).sort_values(['created'], ascending=False)\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Key 'data' not found in response:\", response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4",
   "metadata": {
    "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>active</th>\n",
       "      <th>context_window</th>\n",
       "      <th>public_apps</th>\n",
       "      <th>max_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-86m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632165</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-22m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632101</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen/qwen3-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1748396646</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>40960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meta-llama/llama-guard-4-12b</td>\n",
       "      <td>model</td>\n",
       "      <td>1746743847</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743877158</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743874824</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>compound-beta-mini</td>\n",
       "      <td>model</td>\n",
       "      <td>1742953279</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1741214760</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>compound-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1740880017</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>playai-tts-arabic</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682783</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>playai-tts</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682771</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral-saba-24b</td>\n",
       "      <td>model</td>\n",
       "      <td>1739996492</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>True</td>\n",
       "      <td>32768</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737924940</td>\n",
       "      <td>DeepSeek / Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>allam-2-7b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737672203</td>\n",
       "      <td>SDAIA</td>\n",
       "      <td>True</td>\n",
       "      <td>4096</td>\n",
       "      <td>None</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>model</td>\n",
       "      <td>1733447754</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whisper-large-v3-turbo</td>\n",
       "      <td>model</td>\n",
       "      <td>1728413088</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama3-8b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>whisper-large-v3</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>distil-whisper-large-v3-en</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Hugging Face</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama3-70b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id object     created  \\\n",
       "13            meta-llama/llama-prompt-guard-2-86m  model  1748632165   \n",
       "7             meta-llama/llama-prompt-guard-2-22m  model  1748632101   \n",
       "19                                 qwen/qwen3-32b  model  1748396646   \n",
       "1                    meta-llama/llama-guard-4-12b  model  1746743847   \n",
       "6   meta-llama/llama-4-maverick-17b-128e-instruct  model  1743877158   \n",
       "11      meta-llama/llama-4-scout-17b-16e-instruct  model  1743874824   \n",
       "5                              compound-beta-mini  model  1742953279   \n",
       "21                                   qwen-qwq-32b  model  1741214760   \n",
       "0                                   compound-beta  model  1740880017   \n",
       "10                              playai-tts-arabic  model  1740682783   \n",
       "2                                      playai-tts  model  1740682771   \n",
       "3                                mistral-saba-24b  model  1739996492   \n",
       "14                  deepseek-r1-distill-llama-70b  model  1737924940   \n",
       "12                                     allam-2-7b  model  1737672203   \n",
       "20                        llama-3.3-70b-versatile  model  1733447754   \n",
       "4                          whisper-large-v3-turbo  model  1728413088   \n",
       "8                                  llama3-8b-8192  model  1693721698   \n",
       "9                                whisper-large-v3  model  1693721698   \n",
       "17                     distil-whisper-large-v3-en  model  1693721698   \n",
       "16                           llama-3.1-8b-instant  model  1693721698   \n",
       "15                                llama3-70b-8192  model  1693721698   \n",
       "18                                   gemma2-9b-it  model  1693721698   \n",
       "\n",
       "           owned_by  active  context_window public_apps  max_completion_tokens  \n",
       "13             Meta    True             512        None                    512  \n",
       "7              Meta    True             512        None                    512  \n",
       "19    Alibaba Cloud    True          131072        None                  40960  \n",
       "1              Meta    True          131072        None                   1024  \n",
       "6              Meta    True          131072        None                   8192  \n",
       "11             Meta    True          131072        None                   8192  \n",
       "5              Groq    True          131072        None                   8192  \n",
       "21    Alibaba Cloud    True          131072        None                 131072  \n",
       "0              Groq    True          131072        None                   8192  \n",
       "10           PlayAI    True            8192        None                   8192  \n",
       "2            PlayAI    True            8192        None                   8192  \n",
       "3        Mistral AI    True           32768        None                  32768  \n",
       "14  DeepSeek / Meta    True          131072        None                 131072  \n",
       "12            SDAIA    True            4096        None                   4096  \n",
       "20             Meta    True          131072        None                  32768  \n",
       "4            OpenAI    True             448        None                    448  \n",
       "8              Meta    True            8192        None                   8192  \n",
       "9            OpenAI    True             448        None                    448  \n",
       "17     Hugging Face    True             448        None                    448  \n",
       "16             Meta    True          131072        None                 131072  \n",
       "15             Meta    True            8192        None                   8192  \n",
       "18           Google    True            8192        None                   8192  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "pd.DataFrame(response.json()['data']).sort_values(['created'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438489-c102-434b-872d-e807169deef6",
   "metadata": {
    "id": "2b438489-c102-434b-872d-e807169deef6"
   },
   "source": [
    "The Groq client object enables interaction with the Groq REST API and a chat completion request is made via the client.chat.completions.create method.\n",
    "\n",
    "The most important arguments of the client.chat.completions.create method are the following:\n",
    "* messages: a list of messages (dictionary form) that make up the conversation to date\n",
    "* model: a string indicating which model to use (see [list of models](https://console.groq.com/docs/models))\n",
    "* max_completion_tokens: the maximum number of tokens that are generated in the chat completion\n",
    "* response_format: setting this to `{ \"type\": \"json_object\" }` enables JSON output\n",
    "* seed: sample deterministically as best as possible, though identical outputs each time are not guaranteed\n",
    "* temperature: between 0 and 2 where higher values like 0.8 make the output more random (creative) and values like 0.2 are more focused and deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99216392-acc3-4a00-826b-c166a1e52534",
   "metadata": {
    "id": "99216392-acc3-4a00-826b-c166a1e52534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module groq.resources.chat.completions:\n",
      "\n",
      "create(*, messages: 'Iterable[ChatCompletionMessageParam]', model: \"Union[str, Literal['gemma2-9b-it', 'llama-3.3-70b-versatile', 'llama-3.1-8b-instant', 'llama-guard-3-8b', 'llama3-70b-8192', 'llama3-8b-8192']]\", exclude_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, function_call: 'Optional[completion_create_params.FunctionCall] | NotGiven' = NOT_GIVEN, functions: 'Optional[Iterable[completion_create_params.Function]] | NotGiven' = NOT_GIVEN, include_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN, logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN, max_completion_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, metadata: 'Optional[Dict[str, str]] | NotGiven' = NOT_GIVEN, n: 'Optional[int] | NotGiven' = NOT_GIVEN, parallel_tool_calls: 'Optional[bool] | NotGiven' = NOT_GIVEN, presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, reasoning_effort: \"Optional[Literal['none', 'default']] | NotGiven\" = NOT_GIVEN, reasoning_format: \"Optional[Literal['hidden', 'raw', 'parsed']] | NotGiven\" = NOT_GIVEN, response_format: 'Optional[completion_create_params.ResponseFormat] | NotGiven' = NOT_GIVEN, search_settings: 'Optional[completion_create_params.SearchSettings] | NotGiven' = NOT_GIVEN, seed: 'Optional[int] | NotGiven' = NOT_GIVEN, service_tier: \"Optional[Literal['auto', 'on_demand', 'flex', 'performance']] | NotGiven\" = NOT_GIVEN, stop: 'Union[Optional[str], List[str], None] | NotGiven' = NOT_GIVEN, store: 'Optional[bool] | NotGiven' = NOT_GIVEN, stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN, temperature: 'Optional[float] | NotGiven' = NOT_GIVEN, tool_choice: 'Optional[ChatCompletionToolChoiceOptionParam] | NotGiven' = NOT_GIVEN, tools: 'Optional[Iterable[ChatCompletionToolParam]] | NotGiven' = NOT_GIVEN, top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN, top_p: 'Optional[float] | NotGiven' = NOT_GIVEN, user: 'Optional[str] | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of groq.resources.chat.completions.Completions instance\n",
      "    Creates a model response for the given chat conversation.\n",
      "\n",
      "    Args:\n",
      "      messages: A list of messages comprising the conversation so far.\n",
      "\n",
      "      model: ID of the model to use. For details on which models are compatible with the Chat\n",
      "          API, see available [models](https://console.groq.com/docs/models)\n",
      "\n",
      "      exclude_domains: Deprecated: Use search_settings.exclude_domains instead. A list of domains to\n",
      "          exclude from the search results when the model uses a web search tool.\n",
      "\n",
      "      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n",
      "          existing frequency in the text so far, decreasing the model's likelihood to\n",
      "          repeat the same line verbatim.\n",
      "\n",
      "      function_call: Deprecated in favor of `tool_choice`.\n",
      "\n",
      "          Controls which (if any) function is called by the model. `none` means the model\n",
      "          will not call a function and instead generates a message. `auto` means the model\n",
      "          can pick between generating a message or calling a function. Specifying a\n",
      "          particular function via `{\"name\": \"my_function\"}` forces the model to call that\n",
      "          function.\n",
      "\n",
      "          `none` is the default when no functions are present. `auto` is the default if\n",
      "          functions are present.\n",
      "\n",
      "      functions: Deprecated in favor of `tools`.\n",
      "\n",
      "          A list of functions the model may generate JSON inputs for.\n",
      "\n",
      "      include_domains: Deprecated: Use search_settings.include_domains instead. A list of domains to\n",
      "          include in the search results when the model uses a web search tool.\n",
      "\n",
      "      logit_bias: This is not yet supported by any of our models. Modify the likelihood of\n",
      "          specified tokens appearing in the completion.\n",
      "\n",
      "      logprobs: This is not yet supported by any of our models. Whether to return log\n",
      "          probabilities of the output tokens or not. If true, returns the log\n",
      "          probabilities of each output token returned in the `content` of `message`.\n",
      "\n",
      "      max_completion_tokens: The maximum number of tokens that can be generated in the chat completion. The\n",
      "          total length of input tokens and generated tokens is limited by the model's\n",
      "          context length.\n",
      "\n",
      "      max_tokens: Deprecated in favor of `max_completion_tokens`. The maximum number of tokens\n",
      "          that can be generated in the chat completion. The total length of input tokens\n",
      "          and generated tokens is limited by the model's context length.\n",
      "\n",
      "      metadata: This parameter is not currently supported.\n",
      "\n",
      "      n: How many chat completion choices to generate for each input message. Note that\n",
      "          the current moment, only n=1 is supported. Other values will result in a 400\n",
      "          response.\n",
      "\n",
      "      parallel_tool_calls: Whether to enable parallel function calling during tool use.\n",
      "\n",
      "      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\n",
      "          whether they appear in the text so far, increasing the model's likelihood to\n",
      "          talk about new topics.\n",
      "\n",
      "      reasoning_effort: this field is only available for qwen3 models. Set to 'none' to disable\n",
      "          reasoning. Set to 'default' or null to let Qwen reason.\n",
      "\n",
      "      reasoning_format: Specifies how to output reasoning tokens\n",
      "\n",
      "      response_format: An object specifying the format that the model must output. Setting to\n",
      "          `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs\n",
      "          which ensures the model will match your supplied JSON schema. json_schema\n",
      "          response format is only supported on llama 4 models. Setting to\n",
      "          `{ \"type\": \"json_object\" }` enables the older JSON mode, which ensures the\n",
      "          message the model generates is valid JSON. Using `json_schema` is preferred for\n",
      "          models that support it.\n",
      "\n",
      "      search_settings: Settings for web search functionality when the model uses a web search tool.\n",
      "\n",
      "      seed: If specified, our system will make a best effort to sample deterministically,\n",
      "          such that repeated requests with the same `seed` and parameters should return\n",
      "          the same result. Determinism is not guaranteed, and you should refer to the\n",
      "          `system_fingerprint` response parameter to monitor changes in the backend.\n",
      "\n",
      "      service_tier: The service tier to use for the request. Defaults to `on_demand`.\n",
      "\n",
      "          - `auto` will automatically select the highest tier available within the rate\n",
      "            limits of your organization.\n",
      "          - `flex` uses the flex tier, which will succeed or fail quickly.\n",
      "\n",
      "      stop: Up to 4 sequences where the API will stop generating further tokens. The\n",
      "          returned text will not contain the stop sequence.\n",
      "\n",
      "      store: This parameter is not currently supported.\n",
      "\n",
      "      stream: If set, partial message deltas will be sent. Tokens will be sent as data-only\n",
      "          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n",
      "          as they become available, with the stream terminated by a `data: [DONE]`\n",
      "          message. [Example code](/docs/text-chat#streaming-a-chat-completion).\n",
      "\n",
      "      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n",
      "          make the output more random, while lower values like 0.2 will make it more\n",
      "          focused and deterministic. We generally recommend altering this or top_p but not\n",
      "          both.\n",
      "\n",
      "      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\n",
      "          not call any tool and instead generates a message. `auto` means the model can\n",
      "          pick between generating a message or calling one or more tools. `required` means\n",
      "          the model must call one or more tools. Specifying a particular tool via\n",
      "          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n",
      "          call that tool.\n",
      "\n",
      "          `none` is the default when no tools are present. `auto` is the default if tools\n",
      "          are present.\n",
      "\n",
      "      tools: A list of tools the model may call. Currently, only functions are supported as a\n",
      "          tool. Use this to provide a list of functions the model may generate JSON inputs\n",
      "          for. A max of 128 functions are supported.\n",
      "\n",
      "      top_logprobs: This is not yet supported by any of our models. An integer between 0 and 20\n",
      "          specifying the number of most likely tokens to return at each token position,\n",
      "          each with an associated log probability. `logprobs` must be set to `true` if\n",
      "          this parameter is used.\n",
      "\n",
      "      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\n",
      "          model considers the results of the tokens with top_p probability mass. So 0.1\n",
      "          means only the tokens comprising the top 10% probability mass are considered. We\n",
      "          generally recommend altering this or temperature but not both.\n",
      "\n",
      "      user: A unique identifier representing your end-user, which can help us monitor and\n",
      "          detect abuse.\n",
      "\n",
      "      extra_headers: Send extra headers\n",
      "\n",
      "      extra_query: Add additional query parameters to the request\n",
      "\n",
      "      extra_body: Add additional JSON properties to the request\n",
      "\n",
      "      timeout: Override the client-level default timeout for this request, in seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7",
   "metadata": {
    "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7"
   },
   "source": [
    "As a first example, note how the messages input is given as a list of a dictionaries with `role` and `content` keys. This is in a ChatML format recognised by many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81",
   "metadata": {
    "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models are artificial intelligence (AI) systems that process and generate human-like language. Here's a simplified overview of how they work:\n",
      "\n",
      "1. **Training**: These models are trained on massive amounts of text data, such as books, articles, and websites.\n",
      "2. **Pattern recognition**: The models learn to recognize patterns in language, including grammar, syntax, and semantics.\n",
      "3. **Neural networks**: The models use neural networks, which are complex algorithms that mimic the human brain, to analyze and generate text.\n",
      "4. **Predictive modeling**: When given a prompt or input, the model predicts the next word or character, based on the patterns it has learned.\n",
      "5. **Generation**: The model generates text by iteratively predicting the next word or character, creating a coherent and context-specific output.\n",
      "\n",
      "This process allows large language models to understand and generate human-like language, enabling applications such as language translation, text summarization, and conversational AI.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {   \"role\": \"system\", # sets the persona of the model\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": \"Explain briefly how large language models work\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b60c2-4300-4338-b325-c3ea876c1afe",
   "metadata": {
    "id": "728b60c2-4300-4338-b325-c3ea876c1afe"
   },
   "source": [
    "The output is in Markdown format so the following line formats this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88845bb3-4a3e-403a-93ab-439c46aa1832",
   "metadata": {
    "id": "88845bb3-4a3e-403a-93ab-439c46aa1832"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large language models are artificial intelligence (AI) systems that process and generate human-like language. Here's a simplified overview of how they work:\n",
       "\n",
       "1. **Training**: These models are trained on massive amounts of text data, such as books, articles, and websites.\n",
       "2. **Pattern recognition**: The models learn to recognize patterns in language, including grammar, syntax, and semantics.\n",
       "3. **Neural networks**: The models use neural networks, which are complex algorithms that mimic the human brain, to analyze and generate text.\n",
       "4. **Predictive modeling**: When given a prompt or input, the model predicts the next word or character, based on the patterns it has learned.\n",
       "5. **Generation**: The model generates text by iteratively predicting the next word or character, creating a coherent and context-specific output.\n",
       "\n",
       "This process allows large language models to understand and generate human-like language, enabling applications such as language translation, text summarization, and conversational AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {
    "id": "e1c8c97b-1711-4355-b108-86bed769c109"
   },
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {
    "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4"
   },
   "source": [
    "We start with a llama3-8b-8192, a model using just over 8 billion parameters with at most 8192 tokens produced as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {
    "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2"
   },
   "source": [
    "Here is an article to be summarised from the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {
    "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29"
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake » . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {
    "id": "9754257a-74ff-4747-a18e-7ebfbc82639c"
   },
   "source": [
    "**Exercise:**\n",
    "Summarise the story text using the following three prompts. Use the format given above but here there is no need to set the persona (i.e. only include one dictionary in the messages list when calling `client.chat.completions.create`.) Comment on any differences.\n",
    "\n",
    "1) \"Summarise the following article in 3 sentences.\"\n",
    "\n",
    "2) \"Give me a TL;DR of this text.\"\n",
    "\n",
    "3) \"What's the key takeaway here?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c372155-bdde-45a4-a184-fce4c4e8034c",
   "metadata": {
    "id": "9c372155-bdde-45a4-a184-fce4c4e8034c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " A magnitude 4.2 earthquake struck the San Francisco area on Friday at 4:42 a.m. PT, causing about 2,000 customers to lose power. The earthquake, which is considered \"light\" according to the USGS, was felt for about 50 seconds and caused no reported injuries or damage. Authorities received a surge of calls, mostly from concerned residents, but there were no reports of damage or injury, with the quake being described as \"fairly mild\".\n",
      "Give me a TL;DR of this text.  \n",
      " A magnitude 4.2 earthquake shook the San Francisco area at 4:42am, causing about 2,000 power outages but no reported injuries or damage. The quake was considered \"light\" and lasted about 50 seconds, with people in the area reporting minor concerns and no damage.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake struck the San Francisco area, causing minimal damage and no reported injuries, with about 2,000 customers losing power.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Summarise the following article in 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"]\n",
    "#content will be p + story for p in prompts\n",
    "\n",
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}]\n",
    ")\n",
    "\n",
    "    print(p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df",
   "metadata": {
    "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df"
   },
   "source": [
    "Run the above code again below and note that the answers may differ. This is due to the probabilistic nature of LLM token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14",
   "metadata": {
    "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " Here is a 3 sentence summary of the article:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42am PT on Friday, causing approximately 2,000 customers to lose power. The USGS classified the earthquake as \"light\" and officials reported no major damage or injuries, with people mostly calling to inquire about the quake. The earthquake was felt for about 50 seconds and was centered about two miles east-northeast of Oakland, with people in the area reporting that it caused some minor disruptions such as setting off home alarms.\n",
      "Give me a TL;DR of this text.  \n",
      " A magnitude 4.2 earthquake occurred in the San Francisco area on Friday at 4:42am, causing minimal damage and no reported injuries. The quake was centered near Oakland and left about 2,000 customers without power. The shaking lasted for 50 seconds and was felt indoors, causing some minor disturbances such as breaking dishes and windows, but no major damage was reported.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake, classified as \"light\" by the USGS, struck the San Francisco area, causing minimal damage and no reported injuries, but leaving about 2,000 customers without power and triggering home alarms in Oakland.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}]\n",
    ")\n",
    "\n",
    "    print(p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a39af-3722-4b71-8d70-cbfa63593496",
   "metadata": {
    "id": "405a39af-3722-4b71-8d70-cbfa63593496"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {
    "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f"
   },
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {
    "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf"
   },
   "source": [
    "**Exercise**: In this section adjust the `max_completion_tokens` and `temperature` settings below to obtain different responses. Show some examples with the prompt \"Continue the story: It was a great time to be alive\" with the model \"llama-3.1-8b-instant\".\n",
    "\n",
    "* max_completion_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens (set to 200 and 500)\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output (set to 0.2, 0.8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50207704-94be-4309-8a82-dc1d22a063ee",
   "metadata": {
    "id": "50207704-94be-4309-8a82-dc1d22a063ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, with the warm sun on your skin, the sound of laughter and music floating through the air, and the feeling of possibility and adventure stretching out before you like a tantalizing promise. \n",
      "\n",
      "For Emily, it was a summer like no other. She had just turned 20, and after graduating from college, she had spent the past few months working odd jobs and saving up to embark on a grand adventure. She had always dreamed of traveling the world, of experiencing different cultures, trying new foods, and making new friends.\n",
      "\n",
      "And now, at last, the time had come. Emily packed a small bag, grabbed her backpack, and set off for the airport, where she would meet up with her best friend, Sarah, and begin their journey together.\n",
      "\n",
      "They were bound for Europe, a continent that seemed to hold a special allure for Emily. She had always been fascinated by the history, the art, and the architecture of the old world, and she couldn\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=200, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63235fcc-3a61-4829-998b-37f2f104322b",
   "metadata": {
    "id": "63235fcc-3a61-4829-998b-37f2f104322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, the year was 1969 and the world was buzzing with excitement. The Vietnam War was raging on, but for 20-year-old Sarah, the music, fashion, and culture of the time were a respite from the chaos. She had just left her small town in the Midwest and moved to New York City, ready to chase her dreams and experience the world.\n",
      "\n",
      "As she walked down the streets of Greenwich Village, she was surrounded by the vibrant sounds of Jimi Hendrix and Janis Joplin blaring from the bars and clubs. The air was thick with the smell of incense and patchouli, and the streets were lined with hippies and artists, all pushing the boundaries of fashion and art.\n",
      "\n",
      "Sarah had recently landed a job at a trendy boutique on Bleecker Street, where she sold everything from flowy maxi dresses to handmade jewelry. The store was a hub for the local counterculture, and Sarah was quickly becoming a part of it. She had traded in her conservative dress code for a more carefree, flower-child attitude, and her coworkers were a diverse and eccentric bunch.\n",
      "\n",
      "One evening, as the sun set over the Hudson River, Sarah and her friends decided to sneak into a secret concert at a abandoned warehouse in the Meatpacking District. They had heard rumors of a mysterious group called \"The Black Rose\" who were known to put on epic performances, but only for a select few. The group was shrouded in mystery, and Sarah couldn't help but feel a thrill of excitement at the thought of stumbling upon something truly underground.\n",
      "\n",
      "As they pushed their way through the crowded warehouse, the music grew louder and the air grew thick with anticipation. On stage, a group of musicians were setting up their instruments, including a lead guitarist with a shock of wild hair and a charismatic lead singer with a mesmerizing presence.\n",
      "\n",
      "The music began, and Sarah felt herself transported to a different world. The Black Rose was unlike anything she had ever heard before – their sound was raw and experimental, a blend of rock, folk, and psychedelia that left her breathless.\n",
      "\n",
      "As the night wore on, Sarah found herself drawn to the lead singer, who seemed to be speaking directly to her soul. She felt a connection to him that went beyond words, a sense of understanding that was both familiar and yet completely new.\n",
      "\n",
      "As the concert came to a close, Sarah knew that she had stumbled upon something special – a community\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=500, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens=500,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {
    "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun was shining bright, casting a warm glow over the bustling streets of the city. People of all ages and backgrounds walked side by side, each with their own unique story to tell. The air was filled with the sweet scent of blooming flowers and the sound of laughter and music drifted through the air.\n",
      "\n",
      "It was a great time to be alive, indeed. The world was on the cusp of a new era of peace and prosperity, and everyone could feel it. The wars that had ravaged the planet for so long were finally coming to an end, and the people were beginning to rebuild and reconnect.\n",
      "\n",
      "As I walked through the city, I couldn't help but feel a sense of hope and optimism. Everywhere I looked, I saw people working together, supporting each other, and striving for a better future. It was a truly inspiring sight, and it filled my heart with joy and gratitude.\n",
      "\n",
      "I stopped at a small café to grab a cup of coffee and take in the sights and sounds of the city. As I sat at a small table outside, I struck up a conversation with a young woman named Sophia. She was a artist, and her passion and creativity were infectious. We talked for hours, sharing stories and ideas, and I was struck by her kindness and generosity.\n",
      "\n",
      "As the sun began to set, Sophia and I decided to take a walk along the river. The sky was painted with hues of pink and orange, and the stars were beginning to twinkle in the night sky. We walked in comfortable silence, taking in the beauty of the world around us.\n",
      "\n",
      "It was a great time to be alive, and I felt grateful to be a part of it. The world was full of possibilities, and I knew that anything was possible. As I looked at Sophia, I felt a sense of connection and belonging that I had never felt before. It was as if we were two pieces of a puzzle, fitting together perfectly.\n",
      "\n",
      "As we walked, I realized that this was more than just a great time to be alive. It was a time of connection, of community, of love and acceptance. It was a time when people came together, put aside their differences, and worked towards a common goal. And I knew that I was exactly where I was meant to be.\n",
      "\n",
      "The world was full of beauty and wonder, and I felt grateful to be a part of it. As I looked at Sophia, I knew that I had found a true friend, a kindred spirit who shared my values and my passions. And as we walked into the sunset, I knew that this was just the beginning of an incredible journey, one that would take me to places I never thought possible.\n",
      "\n",
      "It was a great time to be alive, and I was excited to see what the future held.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 0.2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 0.2,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {
    "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun was shining brightly, casting a warm glow over the vibrant city streets. People of all ages and backgrounds hurried about, each with their own unique purpose and story to tell. The air was alive with the hum of music, laughter, and the distant chirping of birds.\n",
      "\n",
      "As I walked through the bustling streets, I couldn't help but feel a sense of excitement and anticipation. It was a great time to be alive, and I was grateful to be a part of it. Everywhere I looked, there were people coming together, celebrating life, love, and the simple joys of existence.\n",
      "\n",
      "I passed by a group of artists gathered on a sidewalk, painting vibrant murals on the walls of a building. Their brushes danced across the canvas, creating a kaleidoscope of colors that seemed to pulse with the rhythm of the city. I watched in awe as they worked, mesmerized by the energy and creativity that seemed to emanate from every brushstroke.\n",
      "\n",
      "As I continued on my way, I stumbled upon a street performer who was playing a lively tune on his guitar. The music was infectious, and I found myself tapping my feet along with the beat. A group of people gathered around, clapping and cheering the performer on, and soon the whole area was alive with music and laughter.\n",
      "\n",
      "It was moments like these that reminded me of the beauty and joy that the world had to offer. Every day was a new opportunity to experience something amazing, to connect with others, and to find magic in the mundane. And as I walked through the city, surrounded by the sights, sounds, and smells of the vibrant streets, I knew that I was exactly where I was meant to be – living in the moment, and savoring every second of this incredible life.\n",
      "\n",
      "Just as I was lost in thought, I heard a voice call out to me. \"Hey, want to grab a cup of coffee and check out this new café that just opened up?\" It was my friend Emma, who had a sparkle in her eye that hinted at a surprise.\n",
      "\n",
      "\"What's so special about this café?\" I asked, intrigued.\n",
      "\n",
      "\"You'll see,\" she said with a mischievous grin. \"Trust me, it's going to be an amazing day.\"\n",
      "\n",
      "I followed Emma through the winding streets, feeling a sense of excitement and curiosity. We turned a corner, and suddenly, the most beautiful café came into view. The exterior was a vibrant mural, painted by the same artists I had seen earlier. We pushed open the door, and a bell above it rang out, welcoming us to a whole new world of flavors and experiences.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 1, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 1,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {
    "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf"
   },
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "931f8e00-928c-4218-8e74-8c56269fbfcb",
   "metadata": {
    "id": "931f8e00-928c-4218-8e74-8c56269fbfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music was everywhere, from the boomboxes blaring in cars on the Sunset-strip in LA to the clubs in London pumping out the latest hits from Duran Duran and Wham!. The smell of freshly poured asphalt and cigarette smoke filled the air as people reveled in the carefree spirit of 1985.\n",
      "\n",
      "You were 25 at the time, with a job in the city as an emerging marketing executive, working closely with one of the largest record labels. Your evenings were spent frequenting rooftop parties above some of the city's hottest clubs and dancing till daylight to the sound of Michael Jackson pouring music out of the speakers.\n",
      "\n",
      "A chance introduction had led you back to college with your old flame, now in its second season working for her fashion degree after a summer job at Bloomingdale's department store downtown, had made you realize love and work were not conflicting forces in  1985 – you could easily be seen having a blast together in New Mexico as you had on the Fourth July that same summer the night they opened a movie theater downtown with a giant poster on which 'American Dreaming' appeared.\n",
      "\n",
      "As it was in the middle of a sweltering summer season on the day which was going to become etched into their memories, in New Mexico. That afternoon was marked in anticipation in the air. The couple went in for lunch where it turned up out unexpectedly how  that was their final farewell date before he moved out on his very short trip the city in LA to begin this promising marketing profession - you would spend a whole evening thinking that this may be your parting date of two friends who may possibly become one for the coming years\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 2,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {
    "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c"
   },
   "source": [
    "### Zero-shot and one-short prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {
    "id": "f6662f0a-c7e3-4235-972d-2771781a4e53"
   },
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {
    "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chemical reactions involve the interaction of two or more substances to form new substances. To understand how two chemicals react, we need to consider several factors:\n",
       "\n",
       "1. **Chemical properties**: Each chemical has its unique properties, such as reactivity, electronegativity, and polarity. These properties determine how the chemical will interact with other substances.\n",
       "2. **Reaction type**: Chemical reactions can be classified into several types, including:\n",
       "\t* **Synthesis reaction**: Combination of two or more substances to form a new compound.\n",
       "\t* **Decomposition reaction**: Breakdown of a single compound into two or more simpler substances.\n",
       "\t* **Single displacement reaction**: One element displaces another element from a compound.\n",
       "\t* **Double displacement reaction**: Two compounds exchange partners to form two new compounds.\n",
       "\t* **Combustion reaction**: Reaction of a substance with oxygen to produce heat and light.\n",
       "3. **Reaction conditions**: The conditions under which the reaction occurs can affect the outcome. These conditions include:\n",
       "\t* **Temperature**: Higher temperatures can increase the rate of reaction.\n",
       "\t* **Pressure**: Increased pressure can increase the rate of reaction.\n",
       "\t* **Catalyst**: Presence of a catalyst can speed up the reaction.\n",
       "\t* **Solvent**: The properties of the solvent can affect the reaction.\n",
       "4. **Chemical equations**: Chemical equations describe the reactants, products, and conditions of the reaction. A balanced equation is a chemical equation that shows the same number of atoms of each element on both the reactant and product sides.\n",
       "\n",
       "To predict how two chemicals will react, you can:\n",
       "\n",
       "1. **Research the properties** of the chemicals involved.\n",
       "2. **Consult chemical databases** or reference materials for information on potential reactions.\n",
       "3. **Conduct experiments** to test the reaction under controlled conditions.\n",
       "4. **Use theoretical models** to predict the reaction outcomes.\n",
       "\n",
       "Some common tools used to predict chemical reactions include:\n",
       "\n",
       "1. **Molecular orbital theory**: This theory helps predict the reactivity of molecules based on their electron configuration.\n",
       "2. **Valence bond theory**: This theory describes the formation of chemical bonds between atoms.\n",
       "3. **Group theory**: This theory helps predict the symmetry of molecules and their reactivity.\n",
       "4. **Quantum mechanics**: This branch of physics helps predict the behavior of atoms and molecules at the molecular level.\n",
       "\n",
       "Keep in mind that predicting chemical reactions is a complex task, and the accuracy of predictions depends on the level of detail and the tools used."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {
    "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d"
   },
   "source": [
    "**Exercise:** Ask the same question but modify the prompt to return the answer to the same question in a simpler form (still using the llama-3.1-8b-instant model). Experiment with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {
    "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have two friends, let's call them Bob and Alice. Bob loves to dance, and Alice loves to sing. When they are together in a big room, they both want to do their favorite thing. But, the room is too quiet, so they can't dance or sing as much as they want.\n",
       "\n",
       "Now, let's say we add some special music to the room. The music makes it loud and fun, and Bob and Alice both start to dance and sing together. But, they don't just dance and sing together, they also start to create something new. They create a new kind of music that's a mix of both of their favorite things.\n",
       "\n",
       "In the same way, when two chemicals (like molecules) meet, they want to do their special job. But, they need some kind of special help, like the music in the room. This special help is called energy. When the chemicals get energy, they start to react. They mix together and create something new, just like Bob and Alice created that new kind of music.\n",
       "\n",
       "This new thing that the chemicals create is called a product. It's like the song that Bob and Alice made together. And just like how Bob and Alice can't just stop singing and dancing in the middle of the song, the chemicals can't just stop reacting once they've started. They keep going until they've finished making the product."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Answer the following question as though I am 10 years old. How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {
    "id": "04676a11-57a7-4f9c-87e2-128559f0ffce"
   },
   "source": [
    "### One-shot prompting ###\n",
    "\n",
    "Next, note the dramatic change when we give the following template setting a new role and providing an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {
    "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment réagissent deux composés chimiques ?\n",
      "\n",
      "(Note: This is a general question, and the answer would depend on the specific chemicals involved. If you provide more context or information about the chemicals, I can give a more detailed and accurate answer.)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"system\",\n",
    "             \"content\": \"You translate English to French.\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What time is it?\"},\n",
    "               {\"role\": \"assistant\",\n",
    "               \"content\": \"Quelle heure est-il?\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {
    "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269"
   },
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {
    "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050"
   },
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. This is where examples can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b",
   "metadata": {
    "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regrettably, I will be unable to attend the meeting.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"I'm gonna head out now, see you later.\"\n",
    "response1 = \"I will be leaving now. See you later.\"\n",
    "\n",
    "prompt2 =  \"That movie was super cool!\"\n",
    "response2 = \"The movie was very impressive.\"\n",
    "\n",
    "prompt3 = \"Can't make it to the meeting, sorry.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional editor. Rewrite casual sentences into a formal tone.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e9e18-cc27-453b-8844-3f683fb607f8",
   "metadata": {
    "id": "040e9e18-cc27-453b-8844-3f683fb607f8"
   },
   "source": [
    "The output can also be moulded to provide SQL output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {
    "id": "ed854534-30db-4745-b910-27d12a3ed47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM products WHERE quantity_in_stock = 0;\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Show me all users who signed up in the last 30 days.\"\n",
    "response1 = \"SELECT * FROM users WHERE signup_date >= CURRENT_DATE - INTERVAL '30 days';\"\n",
    "\n",
    "prompt2 = \"What is the average order value?\"\n",
    "response2 =  \"SELECT AVG(order_total) FROM orders;\"\n",
    "\n",
    "prompt3 = \"List products that are out of stock.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that translates natural language to SQL.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7",
   "metadata": {
    "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7"
   },
   "source": [
    "**Exercise**: Create a few examples to train the \"llama3-70b-8192\" LLM to take in user content in the form below and provide output as a pandas dataframe. Use the `exec` function to execute its output to display the answer of sample input as a data frame.\n",
    "\n",
    "Example:\n",
    "\n",
    "given the user content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 32 | 27 | 25\n",
    "\n",
    "| 64 | 23 | 14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train the model to output\n",
    "\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b8ac08a-5759-41cd-a560-e77694573723",
   "metadata": {
    "id": "2b8ac08a-5759-41cd-a560-e77694573723"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colA</th>\n",
       "      <th>colB</th>\n",
       "      <th>colC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   colA  colB  colC\n",
       "0    23    12    54\n",
       "1     8    76    32\n",
       "2     7     5     3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "\n",
    "user1 = \"\"\"col1 | col2 | col3\n",
    "32 | 27 | 25\n",
    "64 | 23 | 14\n",
    "\"\"\"\n",
    "\n",
    "output1 = \"\"\"\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\"\"\"\n",
    "\n",
    "user2 = \"\"\"col1 | col2\n",
    "23 | 12\n",
    "8 | 76\n",
    "7 | 5\n",
    "\"\"\"\n",
    "output2 = \"\"\"\n",
    "df = pd.DataFrame({'col1': [23, 8, 7], 'col2': [12, 76, 5]})\n",
    "\"\"\"\n",
    "user3 = \"\"\"colA | colB | colC\n",
    "23 | 12 | 54\n",
    "8 | 76 | 32\n",
    "7 | 5 | 3\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a data scientist who will receive data input as a string and provide output as a pandas dataframe called df. Use the examples to guide you\"},\n",
    "        {\"role\": \"user\", \"content\": user1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": user2},\n",
    "        {\"role\": \"user\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": user3}\n",
    "    ]\n",
    ")\n",
    "\n",
    "exec(response.choices[0].message.content.strip()) # string executed as Python code\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f62a88-c7b4-4431-acda-97e9a98981f4",
   "metadata": {
    "id": "29f62a88-c7b4-4431-acda-97e9a98981f4"
   },
   "source": [
    "Also show what happens when the question is asked in the absence of a system role and without few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6",
   "metadata": {
    "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It looks like you've provided a table with three columns: `colA`, `colB`, and `colC`. Here's a formatted version of the table:\\n\\n| colA | colB | colC |\\n| --- | --- | --- |\\n| 23  | 12  | 54  |\\n| 8   | 76  | 32  |\\n| 7   | 5   | 3   |\\n\\nLet me know if you'd like me to perform any operations on this table or answer any questions about it!\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user3}\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {
    "id": "28660884-dfd4-4b96-a65d-d21ddbf99423"
   },
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {
    "id": "724f85ef-afc0-4e31-9ad8-ae780c535837"
   },
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee04ad6-968b-406a-8119-981b01ef5f92",
   "metadata": {
    "id": "bee04ad6-968b-406a-8119-981b01ef5f92"
   },
   "source": [
    "**Exercise**: Using the following prompts, compare the answers of the \"llama3-8b-8192\" model (set seed=21). (If this model is no longer available choose a model with relatively few parameters.)\n",
    "\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb",
   "metadata": {
    "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------zero-shot-prompt------\n",
      "There are 2 s's in the word 'success'.\n",
      "------chain-of-thought------\n",
      "To count the number of 's's in the word \"success\", we'll go through each letter one by one:\n",
      "\n",
      "1. S\n",
      "   There's one 's' in the first position.\n",
      "\n",
      "2. U\n",
      "   Nothing else to add, there's no 's' with this 'u'.\n",
      "\n",
      "3. C\n",
      "   There's no 's' with this 'c' either.\n",
      "\n",
      "4. C\n",
      "   Still not an 's'.\n",
      "\n",
      "5. E\n",
      "   The word doesn't end at 'e' yet, so there's no 's' here.\n",
      "\n",
      "6. S\n",
      "   Now we have a second 's' in the sixth position.\n",
      "\n",
      "7. S\n",
      "   This is the third 's' in the seventh position of the word \"success\".\n",
      "\n",
      "The total number of 's's in the word \"success\" is 3.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
    "    seed = 21\n",
    ")\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": chain_of_thought_prompt}],\n",
    "    seed = 21\n",
    ")\n",
    "\n",
    "print('------zero-shot-prompt------')\n",
    "print(response1.choices[0].message.content)\n",
    "\n",
    "print('------chain-of-thought------')\n",
    "print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f",
   "metadata": {
    "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f"
   },
   "source": [
    "## Comparison of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f",
   "metadata": {
    "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f"
   },
   "source": [
    "**Exercise**: Compare the performance of 2 LLMs by outputting the answers of the following questions into a dataframe.\n",
    "\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "\n",
    "Column headings:\n",
    "\n",
    "Model Name | Question | Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238",
   "metadata": {
    "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist get lost in the woods?\\n\\nBecause they couldn't find the decision tree!  🌳😭  \\n\\n\\nLet me know if you'd like to hear another one! 😄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>Here's a way to calculate 22 * 13 mentally using a combination of doubling and simplifying:\\n\\n1. **Break down 13:** Think of 13 as (10 + 3).\\n\\n2. **Multiply by 22:**  Now you have:  \\n   * 22 * (10 + 3) \\n\\n3. **Distribute:**\\n   *  22 * 10  +  22 * 3\\n\\n4. **Calculate:**\\n   * 220  +  66\\n\\n5. **Add:**\\n   * 286\\n\\n\\nTherefore, 22 * 13 = 286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Bartholomew Buttons, a cherubic baby with a tuft of messy brown hair and eyes like melted chocolate, lay on his chubby tummy, focused on a bright red toy truck inches away. He yanked, his tiny fist grasping at the brightly colored plastic, but it remained tantalisingly out of reach.\\n\\nFor weeks, Bartholomew had been observing the world from his back or on his tummy, mesmerized by everything from the dust motes dancing in the sunlight to the rumbling gut of the big, friendly dog named Dusty who lumbered around the house. But seeing the truck, his determination to reach it burned like the fire in the fireplace he couldn't quite reach.\\n\\nHe arched his back, the effort pulling at his belly and his chubby legs stretched out like a frog. He knew what his sister, Clara, did to get things – those long, wavy slithers she called “crawling.”  He'd watched her, mimicking the movements with his short, stubby limbs until he was dizzy. \\n\\nToday, he felt different. A spark of the unknown, the almost impossible, flickered within him. He took a deep, rasping breath, his little chest rising and falling like a bellows, and pushed.\\n\\nHis tummy muscles strained, his arms flailed, and then… a tiny inch. \\n\\nHe stopped, shocked by the movement, the world tilting ever so slightly.  But then a grin, ear to ear, spread across his face. He realized what he’d done! He had moved!\\n\\nHe pushed again, this time a little further, then another push, and another. He was going! It was slow, clumsy, and his legs wobbled like newborn fawns, but he was moving towards the truck, his eyes fixed on its red glory.\\n\\nSuddenly, the rug bunched under his knee.\\n\\nBartholomew toppled forward, landing on his face with a soft “thump.” For a moment, he lay there, stunned. Then, his bottom lip quivered, a wave of tears threatening to spill. \\n\\nBut then, a chuckle came from behind him, a warm, familiar rumble. He felt himself being lifted, and a pair of brown hands wiped his damp cheek. Dusty, the big, friendly dog, licked his face, his tail thumping against the floor. \\n\\n\\nBartholomew giggled, forgetting his frustrations. He was going to crawl. He knew it. He took a deep breath, mustered his chubby muscles, and pushed, achieving another tiny inch. The journey would be long, the falls plentiful, but Bartholomew Buttons was determined. The world, with its countless red trucks, awaited. He was ready.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist quit his job? \\n\\nBecause he didn't get the right correlation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>To calculate 22 * 13 mentally, you can use the following method:\\n\\n1. Break down 22 into 20 + 2.\\n2. Break down 13 into 10 + 3.\\n3. Multiply the numbers using the distributive property:\\n   (20 * 10) + (20 * 3) + (2 * 10) + (2 * 3)\\n4. Calculate each product:\\n   (200) + (60) + (20) + (6)\\n5. Add the results together:\\n   (200) + (60) + (20) + (6) = 286\\n\\nSo, 22 * 13 equals 286 mentally.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>**The Great Escape**\\n\\nIn a cozy little nursery, surrounded by soft toys and colorful walls, a wiggly bundle of joy was waiting to break free. Luna, a tiny baby with bright brown eyes and chubby cheeks, had been watching her siblings crawl for what felt like an eternity. It was time for her own adventure.\\n\\nLuna's mom, Emma, noticed the spark in her baby's eyes and said, \"Today's the day, little one. Get up and crawl!\" She placed a soft toy just out of reach, enticing Luna to take a step closer.\\n\\nAt first, Luna's attempts were more like flailing than crawling. Her chubby legs waved in the air, like a pair of oversized, pink wings. She giggled and cooed, completely unaware of her clumsy movements. Emma chuckled and encouraged her, \"Come on, Luna! You can do it!\"\\n\\nUndeterred, Luna kept trying. She pushed off with her hands and made a tiny lurch forward, only to tumble onto her tummy. Her face scrunched up in concentration, she wriggled and squirmed, her little body shaking with effort. Emma cheered her on, \"You're so close, Luna! Keep going!\"\\n\\nThen, something miraculous happened. Luna's arms suddenly found a rhythm, pumping back and forth like a tiny little engine. Her legs followed suit, kicking and pushing with newfound strength. To everyone's delight, Luna began to move – slowly at first, but surely – across the floor.\\n\\nAs she crawled, Luna's face lit up with excitement. She discovered a world of textures and sensations: the softness of the rug, the roughness of the carpet, and the gentle give of a stuffed animal under her fingers. Her squeals of delight echoed through the nursery, a symphony of joy.\\n\\nEmma beamed with pride, snapping pictures and capturing every precious moment of this milestone. Her husband, Tom, joined in the celebration, playing a rendition of \"Happy Birthday\" on his guitar. The nursery was filled with music, movement, and laughter.\\n\\nLuna's crawling escapades became a daily routine. She zoomed across the room, leaving a trail of toys and pillows in her wake. Her family watched, awestruck by her progress and her boundless energy.\\n\\nAs the days passed, Luna grew more confident and adventurous. She crawled over couch cushions, through tunnels and into hideaways. Her world expanded, and with it, her sense of wonder.\\n\\nOne sunny afternoon, as Luna crawled across the floor, she reached out with a chubby hand and grasped the soft toy Emma had placed earlier. With a triumphant cry, she pulled herself closer, nestling the toy against her chest. Emma swept her up in a tight hug, exclaiming, \"You did it, little one! You're crawling, and you're unstoppable!\"\\n\\nLuna basked in the praise and adoration, basking in the glow of a newfound confidence. She knew she was on her way to conquering the world – one crawl at a time.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model Name  \\\n",
       "0          gemma2-9b-it   \n",
       "1          gemma2-9b-it   \n",
       "2          gemma2-9b-it   \n",
       "3  llama-3.1-8b-instant   \n",
       "4  llama-3.1-8b-instant   \n",
       "5  llama-3.1-8b-instant   \n",
       "\n",
       "                                                 Question  \\\n",
       "0                      Tell me a joke about data science.   \n",
       "1                 How can one calculate 22 * 13 mentally?   \n",
       "2  Write a creative story about a baby learning to crawl.   \n",
       "3                      Tell me a joke about data science.   \n",
       "4                 How can one calculate 22 * 13 mentally?   \n",
       "5  Write a creative story about a baby learning to crawl.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Why did the data scientist get lost in the woods?\\n\\nBecause they couldn't find the decision tree!  🌳😭  \\n\\n\\nLet me know if you'd like to hear another one! 😄  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Here's a way to calculate 22 * 13 mentally using a combination of doubling and simplifying:\\n\\n1. **Break down 13:** Think of 13 as (10 + 3).\\n\\n2. **Multiply by 22:**  Now you have:  \\n   * 22 * (10 + 3) \\n\\n3. **Distribute:**\\n   *  22 * 10  +  22 * 3\\n\\n4. **Calculate:**\\n   * 220  +  66\\n\\n5. **Add:**\\n   * 286\\n\\n\\nTherefore, 22 * 13 = 286  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                Bartholomew Buttons, a cherubic baby with a tuft of messy brown hair and eyes like melted chocolate, lay on his chubby tummy, focused on a bright red toy truck inches away. He yanked, his tiny fist grasping at the brightly colored plastic, but it remained tantalisingly out of reach.\\n\\nFor weeks, Bartholomew had been observing the world from his back or on his tummy, mesmerized by everything from the dust motes dancing in the sunlight to the rumbling gut of the big, friendly dog named Dusty who lumbered around the house. But seeing the truck, his determination to reach it burned like the fire in the fireplace he couldn't quite reach.\\n\\nHe arched his back, the effort pulling at his belly and his chubby legs stretched out like a frog. He knew what his sister, Clara, did to get things – those long, wavy slithers she called “crawling.”  He'd watched her, mimicking the movements with his short, stubby limbs until he was dizzy. \\n\\nToday, he felt different. A spark of the unknown, the almost impossible, flickered within him. He took a deep, rasping breath, his little chest rising and falling like a bellows, and pushed.\\n\\nHis tummy muscles strained, his arms flailed, and then… a tiny inch. \\n\\nHe stopped, shocked by the movement, the world tilting ever so slightly.  But then a grin, ear to ear, spread across his face. He realized what he’d done! He had moved!\\n\\nHe pushed again, this time a little further, then another push, and another. He was going! It was slow, clumsy, and his legs wobbled like newborn fawns, but he was moving towards the truck, his eyes fixed on its red glory.\\n\\nSuddenly, the rug bunched under his knee.\\n\\nBartholomew toppled forward, landing on his face with a soft “thump.” For a moment, he lay there, stunned. Then, his bottom lip quivered, a wave of tears threatening to spill. \\n\\nBut then, a chuckle came from behind him, a warm, familiar rumble. He felt himself being lifted, and a pair of brown hands wiped his damp cheek. Dusty, the big, friendly dog, licked his face, his tail thumping against the floor. \\n\\n\\nBartholomew giggled, forgetting his frustrations. He was going to crawl. He knew it. He took a deep breath, mustered his chubby muscles, and pushed, achieving another tiny inch. The journey would be long, the falls plentiful, but Bartholomew Buttons was determined. The world, with its countless red trucks, awaited. He was ready.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Why did the data scientist quit his job? \\n\\nBecause he didn't get the right correlation.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    To calculate 22 * 13 mentally, you can use the following method:\\n\\n1. Break down 22 into 20 + 2.\\n2. Break down 13 into 10 + 3.\\n3. Multiply the numbers using the distributive property:\\n   (20 * 10) + (20 * 3) + (2 * 10) + (2 * 3)\\n4. Calculate each product:\\n   (200) + (60) + (20) + (6)\\n5. Add the results together:\\n   (200) + (60) + (20) + (6) = 286\\n\\nSo, 22 * 13 equals 286 mentally.  \n",
       "5  **The Great Escape**\\n\\nIn a cozy little nursery, surrounded by soft toys and colorful walls, a wiggly bundle of joy was waiting to break free. Luna, a tiny baby with bright brown eyes and chubby cheeks, had been watching her siblings crawl for what felt like an eternity. It was time for her own adventure.\\n\\nLuna's mom, Emma, noticed the spark in her baby's eyes and said, \"Today's the day, little one. Get up and crawl!\" She placed a soft toy just out of reach, enticing Luna to take a step closer.\\n\\nAt first, Luna's attempts were more like flailing than crawling. Her chubby legs waved in the air, like a pair of oversized, pink wings. She giggled and cooed, completely unaware of her clumsy movements. Emma chuckled and encouraged her, \"Come on, Luna! You can do it!\"\\n\\nUndeterred, Luna kept trying. She pushed off with her hands and made a tiny lurch forward, only to tumble onto her tummy. Her face scrunched up in concentration, she wriggled and squirmed, her little body shaking with effort. Emma cheered her on, \"You're so close, Luna! Keep going!\"\\n\\nThen, something miraculous happened. Luna's arms suddenly found a rhythm, pumping back and forth like a tiny little engine. Her legs followed suit, kicking and pushing with newfound strength. To everyone's delight, Luna began to move – slowly at first, but surely – across the floor.\\n\\nAs she crawled, Luna's face lit up with excitement. She discovered a world of textures and sensations: the softness of the rug, the roughness of the carpet, and the gentle give of a stuffed animal under her fingers. Her squeals of delight echoed through the nursery, a symphony of joy.\\n\\nEmma beamed with pride, snapping pictures and capturing every precious moment of this milestone. Her husband, Tom, joined in the celebration, playing a rendition of \"Happy Birthday\" on his guitar. The nursery was filled with music, movement, and laughter.\\n\\nLuna's crawling escapades became a daily routine. She zoomed across the room, leaving a trail of toys and pillows in her wake. Her family watched, awestruck by her progress and her boundless energy.\\n\\nAs the days passed, Luna grew more confident and adventurous. She crawled over couch cushions, through tunnels and into hideaways. Her world expanded, and with it, her sense of wonder.\\n\\nOne sunny afternoon, as Luna crawled across the floor, she reached out with a chubby hand and grasped the soft toy Emma had placed earlier. With a triumphant cry, she pulled herself closer, nestling the toy against her chest. Emma swept her up in a tight hug, exclaiming, \"You did it, little one! You're crawling, and you're unstoppable!\"\\n\\nLuna basked in the praise and adoration, basking in the glow of a newfound confidence. She knew she was on her way to conquering the world – one crawl at a time.  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ANSWER\n",
    "pd.set_option('display.max_colwidth', None) # allows wide dataframes to be viewed\n",
    "models = [\"gemma2-9b-it\", \"llama-3.1-8b-instant\"] #can edit this\n",
    "\n",
    "# ANSWER\n",
    "prompts = [\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "]\n",
    "\n",
    "results = {'Model Name': [], 'Question': [], 'Answer': []}\n",
    "\n",
    "for model in models:\n",
    "    for prompt in prompts:\n",
    "        results['Model Name'].append(model)\n",
    "        results['Question'].append(prompt)\n",
    "        try:\n",
    "            output = client.chat.completions.create(model = model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            results['Answer'].append(output.choices[0].message.content.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model}: {e}\")\n",
    "            results['Answer'].append((prompt, \"ERROR\"))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5",
   "metadata": {
    "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5"
   },
   "source": [
    "### Bonus\n",
    "\n",
    "See if you can prompt an LLM to perform sentiment analysis (output 'Positive' or 'Negative' only) on a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a",
   "metadata": {
    "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "input1 = \"I absolutely loved the way the story unfolded.\"\n",
    "output1 = \"Positive\"\n",
    "\n",
    "input2 = \"The food was cold and completely flavorless.\"\n",
    "output2 = \"Negative\"\n",
    "\n",
    "input3 = \"She handled the situation with grace and professionalism.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are amazing at sentiment analysis. Give the sentiment of the next sentence as the examples show.\"},\n",
    "        {\"role\": \"user\", \"content\": input1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": input2},\n",
    "        {\"role\": \"assistant\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": input3},\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {
    "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {
    "id": "f279421d-cbca-4003-941e-c2d2bc2833a8"
   },
   "source": [
    "We worked with a few Large Language Models (LLMs) using Groq and experimented with prompting for summarisation, text completion and question-answering tasks.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting to achieve desired forms of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {
    "id": "94f06d5e-7073-485b-b046-afe839ab844c"
   },
   "source": [
    "## References\n",
    "1. [Groq's prompting guide](https://console.groq.com/docs/prompting)\n",
    "2. [Groq's playground](https://console.groq.com/playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {
    "id": "d61aab4a-1330-4762-9318-5635c3a97aa7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
